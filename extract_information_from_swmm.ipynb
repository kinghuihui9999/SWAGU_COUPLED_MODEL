{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c661ee9-9365-4e22-844e-45d2d6d9f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#批量修改节点名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373172d-aad6-409a-a1f9-c079e9153a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inp_sections_with_encoding(file_path, encoding='latin1'):\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    \n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        for line in file:\n",
    "            if line.startswith('[') and line.endswith(']\\n'):\n",
    "                current_section = line.strip()\n",
    "                sections[current_section] = []\n",
    "            elif current_section is not None:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith(';'):\n",
    "                    sections[current_section].append(line)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def prefix_all_node_names_corrected(sections, prefix='J'):\n",
    "    # Sections where direct node names are mentioned\n",
    "    node_sections = ['[JUNCTIONS]', '[OUTFALLS]', '[COORDINATES]', '[VERTICES]']\n",
    "    \n",
    "    # Update node names in direct sections\n",
    "    for section in node_sections:\n",
    "        if section in sections:\n",
    "            modified_section = []\n",
    "            for line in sections[section]:\n",
    "                parts = line.split()\n",
    "                if not parts[0].startswith(prefix):\n",
    "                    parts[0] = prefix + parts[0]\n",
    "                modified_section.append(' '.join(parts))\n",
    "            sections[section] = modified_section\n",
    "\n",
    "    # Correctly modify the outlet names in [SUBCATCHMENTS]\n",
    "    if '[SUBCATCHMENTS]' in sections:\n",
    "        modified_subcatchments = []\n",
    "        for line in sections['[SUBCATCHMENTS]']:\n",
    "            parts = line.split()\n",
    "            if not parts[2].startswith(prefix):\n",
    "                parts[2] = prefix + parts[2]\n",
    "            modified_subcatchments.append(' '.join(parts))\n",
    "        sections['[SUBCATCHMENTS]'] = modified_subcatchments\n",
    "\n",
    "    # Update start and end nodes in [CONDUITS]\n",
    "    if '[CONDUITS]' in sections:\n",
    "        modified_conduits = []\n",
    "        for line in sections['[CONDUITS]']:\n",
    "            parts = line.split()\n",
    "            if not parts[1].startswith(prefix):\n",
    "                parts[1] = prefix + parts[1]\n",
    "            if not parts[2].startswith(prefix):\n",
    "                parts[2] = prefix + parts[2]\n",
    "            modified_conduits.append(' '.join(parts))\n",
    "        sections['[CONDUITS]'] = modified_conduits\n",
    "\n",
    "    return sections\n",
    "\n",
    "def write_modified_inp(file_path, sections):\n",
    "    with open(file_path, 'w', encoding='latin1') as file:\n",
    "        for section, lines in sections.items():\n",
    "            file.write(section + '\\n')\n",
    "            for line in lines:\n",
    "                file.write(line + '\\n')\n",
    "            file.write('\\n')  # Ensure good section separation\n",
    "\n",
    "# Specify your original and destination file paths\n",
    "inp_file_path = './inpFile.inp'  # Update this path\n",
    "corrected_modified_inp_file_path = './corrected_fully_modifiedFile.inp'  # Update this path\n",
    "\n",
    "# Process\n",
    "sections_encoded = read_inp_sections_with_encoding(inp_file_path)\n",
    "sections_fully_corrected = prefix_all_node_names_corrected(sections_encoded)\n",
    "write_modified_inp(corrected_modified_inp_file_path, sections_fully_corrected)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6f0d92e-4f6c-44d8-988f-e77dd136140c",
   "metadata": {},
   "source": [
    "#运行swmm，统计结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b96a6c-2278-4a9d-a54a-f77480075ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswmm import Simulation, Nodes\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def parse_coordinates(inp_file):\n",
    "    \"\"\" 从 SWMM .inp 文件中解析节点坐标 \"\"\"\n",
    "    coords = {}\n",
    "    with open(inp_file, 'r') as file:\n",
    "        read_coords = False\n",
    "        for line in file:\n",
    "            if line.startswith('[COORDINATES]'):\n",
    "                read_coords = True\n",
    "                continue\n",
    "            if read_coords:\n",
    "                if line.startswith('['):  # 结束坐标部分\n",
    "                    break\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    try:\n",
    "                        node_id, x, y = parts[0], float(parts[1]), float(parts[2])\n",
    "                        coords[node_id] = (x, y)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    return coords\n",
    "\n",
    "def run_simulation_and_collect_data(inp_file):\n",
    "    \"\"\" 运行模拟并收集节点的溢流数据 \"\"\"\n",
    "    node_coords = parse_coordinates(inp_file)\n",
    "\n",
    "    time_steps = np.arange(0, 180 + 1, 5) * 60  # 转换为秒\n",
    "    node_data = {node_id: {'coords': coords, 'overflows': [0]*len(time_steps)} for node_id, coords in node_coords.items()}  # 初始化溢流数组以匹配时间步数\n",
    "\n",
    "    # 运行模拟并收集数据\n",
    "    with Simulation(inp_file) as sim:\n",
    "        time_step_index = 0  # 跟踪当前时间步长的索引\n",
    "        for step in sim:\n",
    "            current_time = sim.current_time\n",
    "            elapsed_time = (current_time - sim.start_time).total_seconds()\n",
    "            # 更新数据直到当前步骤的时间超过下一个预定时间步长\n",
    "            while time_step_index < len(time_steps) and elapsed_time >= time_steps[time_step_index]:\n",
    "                for node_id in node_coords.keys():\n",
    "                    swmm_node = Nodes(sim)[node_id]\n",
    "                    # 更新溢流数据\n",
    "                    node_data[node_id]['overflows'][time_step_index] = swmm_node.flooding\n",
    "                time_step_index += 1\n",
    "\n",
    "    # 筛选出有溢流记录的节点\n",
    "    overflow_nodes = {node_id: data for node_id, data in node_data.items() if any(data['overflows'])}\n",
    "\n",
    "    return overflow_nodes, time_steps.tolist(), node_coords\n",
    "\n",
    "\n",
    "def write_results_to_file(overflow_results, node_coords, time_steps, file_name):\n",
    "    \"\"\" 将结果写入到文本文件中 \"\"\"\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(\"Time Steps (in seconds): {}\\n\".format(time_steps))\n",
    "        file.write(\"Overflow Nodes:\\n\")\n",
    "        for node_id, data in overflow_results.items():\n",
    "            file.write(\"{}: Overflows: {}\\n\".format(node_id, data['overflows']))\n",
    "\n",
    "        if overflow_results:\n",
    "            file.write(\"\\nNode Coordinates:\\n\")\n",
    "            for node_id in overflow_results.keys():\n",
    "                file.write(\"{}: Coordinates: {}\\n\".format(node_id, node_coords[node_id]))\n",
    "\n",
    "# 调用函数并传入你的 SWMM 模型文件路径\n",
    "inp_file = './inpFile.inp'  # 替换为您的模型文件路径\n",
    "overflow_results, time_steps, node_coords = run_simulation_and_collect_data(inp_file)\n",
    "\n",
    "# 将结果写入文件\n",
    "if overflow_results:\n",
    "    file_name = 'overflow_results_test.txt'\n",
    "    write_results_to_file(overflow_results, node_coords, time_steps, file_name)\n",
    "    print(f\"Results have been written to {file_name}\")\n",
    "else:\n",
    "    print(\"No overflows occurred.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c612f840-f434-42ef-995c-1fec140434d0",
   "metadata": {},
   "source": [
    "#整理成lisflood所需要的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6865c0-e564-4012-80e5-0686dca1992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to parse overflow results from the content string\n",
    "def parse_overflow_results(content):\n",
    "    # Find all node overflow entries\n",
    "    overflow_entries = re.findall(r'(J\\d+): Overflows: \\[([0-9.,\\s]+)\\]', content)\n",
    "    \n",
    "    # Initialize dictionary to store parsed results\n",
    "    parsed_results = {}\n",
    "    \n",
    "    # Process each entry\n",
    "    for node, overflows_str in overflow_entries:\n",
    "        # Convert the string of overflows into a list of floats\n",
    "        overflows = [float(x)/float(17*17) for x in overflows_str.split(', ')]\n",
    "        parsed_results[node] = overflows\n",
    "    \n",
    "    return parsed_results\n",
    "\n",
    "# Function to write formatted data to a new file with spacing between nodes\n",
    "def write_formatted_data_with_spacing(filepath, nodes, time_steps):\n",
    "    with open(filepath, 'w') as file:\n",
    "        # Write the header\n",
    "        file.write('points                                  \\n')\n",
    "        \n",
    "        # Loop through each node to write its data with an extra newline for spacing\n",
    "        for node, overflows in nodes.items():\n",
    "            # Ensure the number of overflows match the time steps, if not append 0's to make them match\n",
    "            if len(overflows) < len(time_steps):\n",
    "                overflows.extend([0.0] * (len(time_steps) - len(overflows)))\n",
    "            \n",
    "            file.write(f'{node}                                  \\n')  # Node name\n",
    "            file.write(f'{len(time_steps)}                                seconds\\n')  # Number of time steps\n",
    "            for time_step, overflow in zip(time_steps, overflows):\n",
    "                file.write(f'                 {overflow}                 {time_step}\\n')  # Overflow value and time step\n",
    "            file.write('\\n')  # Newline for spacing between nodes\n",
    "\n",
    "# Read the content of the newly uploaded overflow results file\n",
    "file_path_new = './overflow_results_test.txt'\n",
    "with open(file_path_new, 'r') as file:\n",
    "    new_overflow_content = file.read()\n",
    "\n",
    "# Extract the time steps from the new content\n",
    "time_steps_match_new = re.search(r'Time Steps \\(in seconds\\): \\[([0-9.,\\s]+)\\]', new_overflow_content)\n",
    "time_steps_new = [int(x) for x in time_steps_match_new.group(1).split(', ')] if time_steps_match_new else []\n",
    "\n",
    "# Parse the new overflow content to get a structured representation\n",
    "parsed_overflow_nodes_new = parse_overflow_results(new_overflow_content)\n",
    "\n",
    "# Generate a new file path for the updated data based on the new file\n",
    "new_file_path_updated_new = './overflow_results_updated_new_formatted.bdy'\n",
    "\n",
    "# Write the formatted data to the new file with the correct time steps and spacing based on the new file\n",
    "write_formatted_data_with_spacing(new_file_path_updated_new, parsed_overflow_nodes_new, time_steps_new)\n",
    "\n",
    "# Output the path for verification\n",
    "print(f'Formatted file created at: {new_file_path_updated_new}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
